{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "nOBMk3qhcKZT"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Installing and Importing Required Libraries"
      ],
      "metadata": {
        "id": "nOBMk3qhcKZT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94pSWwRWb9bv"
      },
      "outputs": [],
      "source": [
        "pip install librosa"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import librosa.display\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import tensorflow as tf\n",
        "from google.colab import drive\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n"
      ],
      "metadata": {
        "id": "XuWdPV-4cFyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the Preprocessed File and Checking the Labels"
      ],
      "metadata": {
        "id": "XfN_bDQ9dAeU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3W7HD1xedJo7",
        "outputId": "e56272d0-92d8-4b88-8f55-be02021928be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the CSV file\n",
        "file_path = '/content/drive/My Drive/deepvoiceguard/dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Checking for any missing labels\n",
        "missing_labels = df[df['label'].isna()]\n",
        "\n",
        "if not missing_labels.empty:\n",
        "    print(\"There are files with missing labels:\")\n",
        "    print(missing_labels)\n",
        "else:\n",
        "    print(\"All files are labeled correctly as 'spoof' or 'bona-fide'.\")\n",
        "\n",
        "# Displaying the distribution of the labels\n",
        "label_distribution = df['label'].value_counts()\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(label_distribution)"
      ],
      "metadata": {
        "id": "n9SNgmqidNL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Having the File paths of all files wrt to the CSV file and labeling as SPoof and Bonafide"
      ],
      "metadata": {
        "id": "tuMXU3Xxdejz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = []\n",
        "speaker = []\n",
        "label = []\n",
        "\n",
        "for i in range(len(df)):\n",
        "  path = f\"/content/drive/MyDrive/deepvoiceguard/preprocessed_dataset/{df['file'][i]}.wav\"\n",
        "  file_path.append(path)\n",
        "  speaker.append(df['speaker'][i])\n",
        "  label.append(df['label'][i])"
      ],
      "metadata": {
        "id": "pOutFqEqdab9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To check if the Lists Match the CSV File Format\n",
        "print(file_path[8],speaker[8],label[8])"
      ],
      "metadata": {
        "id": "-JhFIwXudu1A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting Features and Creating Spectrograms"
      ],
      "metadata": {
        "id": "TPEC5kdWd-EC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " In this project, we are extracting three primary features: MFCC, Chroma, and Zero Crossing Rate.\n",
        "\n",
        " MFCCs are a representation of the short-term power spectrum of a sound signal.\n",
        "\n",
        " Chroma features, or chromagrams, represent the 12 different pitch classes (semitones) of the musical octave. Each pitch class corresponds to a specific frequency range, regardless of the octave.\n",
        "\n",
        " ZCR is the rate at which the audio signal changes sign from positive to negative or vice versa. It is a measure of the frequency content of the signal.\n",
        "\n",
        "\n",
        " By combining MFCC, Chroma, and Zero Crossing Rate features, we can capture a comprehensive set of characteristics from the audio signal. MFCCs provide detailed information about the spectral properties, Chroma features capture harmonic content, and ZCR gives insights into the frequency content and noisiness. Together, these features form a robust foundation for training machine learning models to recognize and classify different types of audio signals, whether it be for spoof detection, speaker recognition, or other audio analysis tasks."
      ],
      "metadata": {
        "id": "9JKyfKcqeAkK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for extracting MFCC, Chroma, and ZCR features\n",
        "def extract_features(file_path, duration=3, sr=22050, n_mfcc=13):\n",
        "    try:\n",
        "        # Load audio file\n",
        "        y, sr = librosa.load(file_path, sr=sr, duration=duration)\n",
        "\n",
        "        # Extract MFCC features\n",
        "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=n_mfcc)\n",
        "        mfccs_mean = np.mean(mfccs.T, axis=0)\n",
        "\n",
        "        # Extract Chroma features\n",
        "        chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
        "        chroma_mean = np.mean(chroma.T, axis=0)\n",
        "\n",
        "        # Extract Zero Crossing Rate\n",
        "        zcr = librosa.feature.zero_crossing_rate(y)\n",
        "        zcr_mean = np.mean(zcr)\n",
        "\n",
        "        # Combine features\n",
        "        features = np.hstack((mfccs_mean, chroma_mean, zcr_mean))\n",
        "\n",
        "        return features\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error encountered while parsing file: {file_path}\")\n",
        "        print(str(e))\n",
        "        return None"
      ],
      "metadata": {
        "id": "0nC1a0e7efhh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the Features and saving the array of features and the Label in a List named \"data\"\n",
        "data = []\n",
        "c=0\n",
        "for i in range(len(df)) :\n",
        "    try:\n",
        "        features = extract_features(file_path[i])\n",
        "        if features is not None:\n",
        "            data.append([features,label[i]])\n",
        "            print(f\"file {i} extraction Done....\\n\")\n",
        "        else:\n",
        "            c+=1\n",
        "            print(f\"Failed to extract features for file: {file_path[i]}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing file: {file_path[i]}\")\n",
        "        print(str(e))"
      ],
      "metadata": {
        "id": "KZXV7t8ze1in"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory to save spectrogram images\n",
        "spectrogram_dir = '/content/spectrograms'\n",
        "os.makedirs(spectrogram_dir, exist_ok=True)\n",
        "\n",
        "def create_spectrogram(file_path, output_path):\n",
        "    try:\n",
        "        # Load audio file\n",
        "        y, sr = librosa.load(file_path, sr=22050)\n",
        "\n",
        "        # Compute Mel spectrogram\n",
        "        S = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "        S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "\n",
        "        # Create a plot\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "        plt.title('Mel-frequency spectrogram')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save plot to file\n",
        "        plt.savefig(output_path)\n",
        "        plt.close()\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating spectrogram for {file_path}: {str(e)}\")\n",
        "\n",
        "# Assuming `file_path` is a list of file paths\n",
        "for i in range(len(df)):\n",
        "    files_path = file_path[i]\n",
        "    output_path = os.path.join(spectrogram_dir, f\"{i}.png\")\n",
        "    create_spectrogram(files_path, output_path)\n"
      ],
      "metadata": {
        "id": "2587sfPjmCFC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Extracting Features To Create a Dataset and Train Models"
      ],
      "metadata": {
        "id": "XBNBCvCv1tbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting features and labels\n",
        "X = np.array([np.hstack(features) for features, _ in data])\n",
        "y = np.array([label for _, label in data])\n",
        "\n",
        "# Encoding labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Spliting data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "metadata": {
        "id": "U2MAs37J0VCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Random Forest Classifier"
      ],
      "metadata": {
        "id": "fd9mz2-z1_vN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for Random Forest\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'max_depth': [10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "rf_grid = GridSearchCV(RandomForestClassifier(), rf_param_grid, refit=True, verbose=2, cv=5)\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters for Random Forest: {rf_grid.best_params_}\")\n",
        "rf_best = rf_grid.best_estimator_\n"
      ],
      "metadata": {
        "id": "iyKbBccz3Z1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate Random Forest model\n",
        "y_pred_rf = rf_best.predict(X_test)\n",
        "print(\"Random Forest Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "print(\"Random Forest Accuracy:\", accuracy_score(y_test, y_pred_rf))\n"
      ],
      "metadata": {
        "id": "lpcPW6T82Cqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##SVM"
      ],
      "metadata": {
        "id": "DYS9GTh92JSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter tuning for SVM\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['rbf', 'linear']\n",
        "}\n",
        "\n",
        "svm_grid = GridSearchCV(SVC(), svm_param_grid, refit=True, verbose=2, cv=5)\n",
        "svm_grid.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best parameters for SVM: {svm_grid.best_params_}\")\n",
        "svm_best = svm_grid.best_estimator_"
      ],
      "metadata": {
        "id": "ht0STLjy3Vzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate SVM model\n",
        "y_pred_svm = svm_best.predict(X_test)\n",
        "\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
        "\n"
      ],
      "metadata": {
        "id": "fyPDaZtq2dAC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Using Models like KNN , XGB and GBM.... to check if they can Give More Accuracy When Compared to SVM and RF"
      ],
      "metadata": {
        "id": "aCfzHmp45en5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to perform Grid Search and return the best model\n",
        "def grid_search_model(model, param_grid, X_train, y_train):\n",
        "    grid_search = GridSearchCV(model, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
        "    grid_search.fit(X_train, y_train)\n",
        "    print(f\"Best parameters for {model.__class__.__name__}: {grid_search.best_params_}\")\n",
        "    return grid_search.best_estimator_\n",
        "\n",
        "# KNN\n",
        "knn_param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance']\n",
        "}\n",
        "knn_best = grid_search_model(KNeighborsClassifier(), knn_param_grid, X_train, y_train)\n",
        "\n",
        "# Gradient Boosting\n",
        "gbm_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "gbm_best = grid_search_model(GradientBoostingClassifier(), gbm_param_grid, X_train, y_train)\n",
        "\n",
        "# XGBoost\n",
        "xgb_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.1, 0.2],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "xgb_best = grid_search_model(XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), xgb_param_grid, X_train, y_train)\n",
        "\n",
        "# Evaluate all models\n",
        "models = {\n",
        "    \"KNN\": knn_best,\n",
        "    \"Gradient Boosting\": gbm_best,\n",
        "    \"XGBoost\": xgb_best\n",
        "}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"\\n{model_name} Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(f\"{model_name} Accuracy: {accuracy_score(y_test, y_pred)}\")\n"
      ],
      "metadata": {
        "id": "3c7e8nIF5AEh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}